
# Concept: Distributed Training and Parallelism

### 1. Définition intuitive
- 

### 2. Définition formelle (si applicable)
- 

**Data Parallelism**: Splits the batch across devices (1 model/GPU).
**Pipeline Parallelism**: Splits the model layers across devices.
**Tensor Parallelism**: Splits individual weight matrices across devices.
**Sequence Parallelism** : split a sequence between GPUs 


### 3. Contextes où il apparaît
- Cours :
  - [[]]
  
- Axes transverses :
  - [[]]

### 4. Liens avec d'autres concepts
- [[]]
- 

### 5. Exemple simple
- 

### 6. Remarques perso / pièges
- 
